{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> ALTeGraD 2023 Data Challenge  - MVA 2023/2024\n",
    "<center> Molecule Retrieval with Natural Language Queries\n",
    "<center> Lilian Hunout $~~$ lilian.hunout@ens-paris-saclay.fr\n",
    "<center> Samy Hocine $~~$ samy.hocine@ens-paris-saclay.fr\n",
    "<center> Lucas Haubert $~~$ lucas.haubert@ens-paris-saclay.fr\n",
    "<center> January 4, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T14:17:41.716940Z",
     "iopub.status.busy": "2024-02-04T14:17:41.716556Z",
     "iopub.status.idle": "2024-02-04T14:17:56.149587Z",
     "shell.execute_reply": "2024-02-04T14:17:56.148507Z",
     "shell.execute_reply.started": "2024-02-04T14:17:41.716910Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T14:17:56.152071Z",
     "iopub.status.busy": "2024-02-04T14:17:56.151736Z",
     "iopub.status.idle": "2024-02-04T14:18:02.679834Z",
     "shell.execute_reply": "2024-02-04T14:18:02.678960Z",
     "shell.execute_reply.started": "2024-02-04T14:17:56.152041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from transformers import AutoConfig\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv, GINConv, global_mean_pool\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-04T14:18:02.681429Z",
     "iopub.status.busy": "2024-02-04T14:18:02.680972Z",
     "iopub.status.idle": "2024-02-04T14:18:02.695341Z",
     "shell.execute_reply": "2024-02-04T14:18:02.694437Z",
     "shell.execute_reply.started": "2024-02-04T14:18:02.681388Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# change directory\n",
    "%cd /kaggle/input/altegrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T14:18:02.696937Z",
     "iopub.status.busy": "2024-02-04T14:18:02.696650Z",
     "iopub.status.idle": "2024-02-04T14:18:02.732176Z",
     "shell.execute_reply": "2024-02-04T14:18:02.731172Z",
     "shell.execute_reply.started": "2024-02-04T14:18:02.696913Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GraphTextDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        gt,\n",
    "        split,\n",
    "        processed,\n",
    "        tokenizer=None,\n",
    "        transform=None,\n",
    "        pre_transform=None,\n",
    "    ):\n",
    "        self.root = root\n",
    "        self.gt = gt\n",
    "        self.split = split\n",
    "        self.processed = processed\n",
    "        self.tokenizer = tokenizer\n",
    "        self.description = (\n",
    "            pd.read_csv(os.path.join(self.root, f\"{split}.tsv\"), sep=\"\\t\", header=None)\n",
    "            .set_index(0)\n",
    "            .to_dict()\n",
    "        )\n",
    "        self.cids = list(self.description[1].keys())\n",
    "        self.idx_to_cid = {i: cid for i, cid in enumerate(self.cids)}\n",
    "        super(GraphTextDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [f\"{cid}.graph\" for cid in self.cids]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f\"data_{cid}.pt\" for cid in self.cids]\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return osp.join(self.root, \"raw\")\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return osp.join(self.processed, \"processed\", self.split)\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process_graph(self, raw_path):\n",
    "        edge_index = []\n",
    "        x = []\n",
    "        with open(raw_path, \"r\") as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                if line != \"\\n\":\n",
    "                    edge = tuple(map(int, line.split()))\n",
    "                    edge_index.append(edge)\n",
    "                else:\n",
    "                    break\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                substruct_id = line.strip().split()[-1]\n",
    "                x.append(self.gt.get(substruct_id, self.gt[\"UNK\"]))\n",
    "        return torch.tensor(edge_index).t().long(), torch.tensor(x).float()\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        for raw_path in self.raw_paths:\n",
    "            cid = int(os.path.basename(raw_path)[:-6])\n",
    "            text_input = self.tokenizer(\n",
    "                [self.description[1][cid]],\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                padding=\"max_length\",\n",
    "                add_special_tokens=True,\n",
    "            )\n",
    "            edge_index, x = self.process_graph(raw_path)\n",
    "            data = Data(\n",
    "                x=x,\n",
    "                edge_index=edge_index,\n",
    "                input_ids=text_input[\"input_ids\"],\n",
    "                attention_mask=text_input[\"attention_mask\"],\n",
    "            )\n",
    "            data_list.append((cid, data))\n",
    "        os.makedirs(self.processed_dir, exist_ok=True)\n",
    "        for cid, data in data_list:\n",
    "            torch.save(data, osp.join(self.processed_dir, f\"data_{cid}.pt\"))\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(\n",
    "            osp.join(self.processed_dir, f\"data_{self.idx_to_cid[idx]}.pt\")\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    def get_cid(self, cid):\n",
    "        data = torch.load(osp.join(self.processed_dir, f\"data_{cid}.pt\"))\n",
    "        return data\n",
    "\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, root, gt, split, processed, transform=None, pre_transform=None):\n",
    "        self.root = root\n",
    "        self.gt = gt\n",
    "        self.split = split\n",
    "        self.processed = processed\n",
    "        self.description = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{split}.txt\"), sep=\"\\t\", header=None\n",
    "        )\n",
    "        self.cids = self.description[0].tolist()\n",
    "        self.idx_to_cid = {i: cid for i, cid in enumerate(self.cids)}\n",
    "        super(GraphDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [f\"{cid}.graph\" for cid in self.cids]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f\"data_{cid}.pt\" for cid in self.cids]\n",
    "\n",
    "    @property\n",
    "    def raw_dir(self) -> str:\n",
    "        return osp.join(self.root, \"raw\")\n",
    "\n",
    "    @property\n",
    "    def processed_dir(self) -> str:\n",
    "        return osp.join(self.processed, \"processed\", self.split)\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process_graph(self, raw_path):\n",
    "        edge_index = []\n",
    "        x = []\n",
    "        with open(raw_path, \"r\") as f:\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                if line != \"\\n\":\n",
    "                    edge = tuple(map(int, line.split()))\n",
    "                    edge_index.append(edge)\n",
    "                else:\n",
    "                    break\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                substruct_id = line.strip().split()[-1]\n",
    "                x.append(self.gt.get(substruct_id, self.gt[\"UNK\"]))\n",
    "        return torch.tensor(edge_index).t().long(), torch.tensor(x).float()\n",
    "\n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        for raw_path in self.raw_paths:\n",
    "            cid = int(os.path.basename(raw_path)[:-6])\n",
    "            edge_index, x = self.process_graph(raw_path)\n",
    "            data = Data(x=x, edge_index=edge_index)\n",
    "            data_list.append((cid, data))\n",
    "        os.makedirs(self.processed_dir, exist_ok=True)\n",
    "        for cid, data in data_list:\n",
    "            torch.save(data, osp.join(self.processed_dir, f\"data_{cid}.pt\"))\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(\n",
    "            osp.join(self.processed_dir, f\"data_{self.idx_to_cid[idx]}.pt\")\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    def get_cid(self, cid):\n",
    "        data = torch.load(osp.join(self.processed_dir, f\"data_{cid}.pt\"))\n",
    "        return data\n",
    "\n",
    "    def get_idx_to_cid(self):\n",
    "        return self.idx_to_cid\n",
    "\n",
    "\n",
    "class TextDataset(TorchDataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.sentences = self.load_sentences(file_path)\n",
    "\n",
    "    def load_sentences(self, file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "        return [line.strip() for line in lines]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first model\n",
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(self, num_node_features, nout, nhid, graph_hidden_channels):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self.nhid = nhid\n",
    "        self.nout = nout\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ln = nn.LayerNorm((nout))\n",
    "        self.conv1 = GCNConv(num_node_features, graph_hidden_channels)\n",
    "        self.conv2 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n",
    "        self.conv3 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n",
    "        self.mol_hidden1 = nn.Linear(graph_hidden_channels, nhid)\n",
    "        self.mol_hidden2 = nn.Linear(nhid, nout)\n",
    "\n",
    "    def forward(self, graph_batch):\n",
    "        x = graph_batch.x\n",
    "        edge_index = graph_batch.edge_index\n",
    "        batch = graph_batch.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.mol_hidden1(x).relu()\n",
    "        x = self.mol_hidden2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T14:18:02.735658Z",
     "iopub.status.busy": "2024-02-04T14:18:02.734841Z",
     "iopub.status.idle": "2024-02-04T14:18:02.771713Z",
     "shell.execute_reply": "2024-02-04T14:18:02.770801Z",
     "shell.execute_reply.started": "2024-02-04T14:18:02.735631Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, num_node_features, nout, nhid, graph_hidden_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.nhid = nhid\n",
    "        self.nout = nout\n",
    "        self.conv1 = GCNConv(num_node_features, graph_hidden_channels)\n",
    "        self.conv2 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n",
    "        self.conv3 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n",
    "        self.bn1 = nn.BatchNorm1d(graph_hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(graph_hidden_channels)\n",
    "        self.bn3 = nn.BatchNorm1d(graph_hidden_channels)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.mol_hidden1 = nn.Linear(graph_hidden_channels, nhid)\n",
    "        self.mol_hidden2 = nn.Linear(nhid, nout)\n",
    "        self.ln = nn.LayerNorm(nout)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, graph_batch):\n",
    "        x = graph_batch.x\n",
    "        edge_index = graph_batch.edge_index\n",
    "        batch = graph_batch.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.mol_hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.mol_hidden2(x)\n",
    "        x = self.ln(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GATEncoder(nn.Module):\n",
    "    def __init__(self, num_node_features, nout, nhid, graph_hidden_channels, heads=2):\n",
    "        super(GATEncoder, self).__init__()\n",
    "        self.nhid = nhid\n",
    "        self.nout = nout\n",
    "        self.conv1 = GATv2Conv(num_node_features, graph_hidden_channels, heads)\n",
    "        self.conv2 = GATv2Conv(\n",
    "            graph_hidden_channels * heads, graph_hidden_channels, heads\n",
    "        )\n",
    "        self.conv3 = GATv2Conv(\n",
    "            graph_hidden_channels * heads, graph_hidden_channels, heads\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(graph_hidden_channels * heads)\n",
    "        self.bn2 = nn.BatchNorm1d(graph_hidden_channels * heads)\n",
    "        self.bn3 = nn.BatchNorm1d(graph_hidden_channels * heads)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.mol_hidden1 = nn.Linear(graph_hidden_channels * heads, nhid)\n",
    "        self.mol_hidden2 = nn.Linear(nhid, nout)\n",
    "        self.ln = nn.LayerNorm(nout)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, graph_batch):\n",
    "        x = graph_batch.x\n",
    "        edge_index = graph_batch.edge_index\n",
    "        batch = graph_batch.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.mol_hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.mol_hidden2(x)\n",
    "        x = self.ln(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GINEncoder(nn.Module):\n",
    "    def __init__(self, num_node_features, nout, nhid, graph_hidden_channels):\n",
    "        super(GINEncoder, self).__init__()\n",
    "        self.nhid = nhid\n",
    "        self.nout = nout\n",
    "        self.conv1 = GINConv(nn.Linear(num_node_features, graph_hidden_channels))\n",
    "        self.conv2 = GINConv(nn.Linear(graph_hidden_channels, graph_hidden_channels))\n",
    "        self.conv3 = GINConv(nn.Linear(graph_hidden_channels, graph_hidden_channels))\n",
    "        self.bn1 = nn.BatchNorm1d(graph_hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(graph_hidden_channels)\n",
    "        self.bn3 = nn.BatchNorm1d(graph_hidden_channels)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.mol_hidden1 = nn.Linear(graph_hidden_channels, nhid)\n",
    "        self.mol_hidden2 = nn.Linear(nhid, nout)\n",
    "        self.ln = nn.LayerNorm(nout)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, graph_batch):\n",
    "        x = graph_batch.x\n",
    "        edge_index = graph_batch.edge_index\n",
    "        batch = graph_batch.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.mol_hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.mol_hidden2(x)\n",
    "        x = self.ln(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        encoded_text = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        # print(encoded_text.last_hidden_state.size())\n",
    "        return encoded_text.last_hidden_state[:, 0, :]\n",
    "\n",
    "\n",
    "# To freeze part of the model\n",
    "\n",
    "# distilbert\n",
    "# class TextEncoder(nn.Module):\n",
    "#     def __init__(self, model_name, freeze_blocks=5):\n",
    "#         super(TextEncoder, self).__init__()\n",
    "#         self.bert = AutoModel.from_pretrained(model_name)\n",
    "#         self.freeze_blocks = freeze_blocks\n",
    "\n",
    "#         # Freeze layers\n",
    "#         for layer in self.bert.transformer.layer[:self.freeze_blocks]:\n",
    "#             for param in layer.parameters():\n",
    "#                 param.requires_grad = False\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         encoded_text = self.bert(input_ids, attention_mask=attention_mask)\n",
    "#         return encoded_text.last_hidden_state[:,0,:]\n",
    "\n",
    "\n",
    "# scibert\n",
    "# class TextEncoder(nn.Module):\n",
    "#     def __init__(self, model_name, freeze_layers=False):\n",
    "#         super(TextEncoder, self).__init__()\n",
    "#         self.bert = AutoModel.from_pretrained(model_name)\n",
    "#         if freeze_layers:\n",
    "#             for param in self.bert.parameters():\n",
    "#                 param.requires_grad = False\n",
    "#             for param in self.bert.encoder.layer[-1].parameters():\n",
    "#                 param.requires_grad = True\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         encoded_text = self.bert(input_ids, attention_mask=attention_mask)\n",
    "#         return encoded_text.last_hidden_state[:,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_name, num_node_features, nout, nhid, graph_hidden_channels\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "        self.graph_encoder = GCNEncoder(\n",
    "            num_node_features, nout, nhid, graph_hidden_channels\n",
    "        )\n",
    "        self.text_encoder = TextEncoder(model_name)\n",
    "\n",
    "    def forward(self, graph_batch, input_ids, attention_mask):\n",
    "        graph_encoded = self.graph_encoder(graph_batch)\n",
    "        text_encoded = self.text_encoder(input_ids, attention_mask)\n",
    "        return graph_encoded, text_encoded\n",
    "\n",
    "    def get_text_encoder(self):\n",
    "        return self.text_encoder\n",
    "\n",
    "    def get_graph_encoder(self):\n",
    "        return self.graph_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T14:18:02.773522Z",
     "iopub.status.busy": "2024-02-04T14:18:02.773183Z",
     "iopub.status.idle": "2024-02-04T14:19:04.960055Z",
     "shell.execute_reply": "2024-02-04T14:19:04.959003Z",
     "shell.execute_reply.started": "2024-02-04T14:18:02.773486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    [\"distilbert-base-uncased\", 768],\n",
    "    [\"WhereIsAI/UAE-Large-V1\", 1024],\n",
    "    [\"allenai/scibert_scivocab_uncased\", 768],\n",
    "    [\"GT4SD/multitask-text-and-chemistry-t5-base-augm\", 768],\n",
    "]\n",
    "model_name, nout = models[0]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "# Access the hidden size\n",
    "hidden_size = config.hidden_size\n",
    "print(f\"Hidden Size: {hidden_size}\")\n",
    "\n",
    "gt = np.load(\"./data/token_embedding_dict.npy\", allow_pickle=True)[()]\n",
    "val_dataset = GraphTextDataset(\n",
    "    root=\"./data/\",\n",
    "    gt=gt,\n",
    "    split=\"val\",\n",
    "    processed=\"/kaggle/working/data/\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "train_dataset = GraphTextDataset(\n",
    "    root=\"./data/\",\n",
    "    gt=gt,\n",
    "    split=\"train\",\n",
    "    processed=\"/kaggle/working/data/\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T14:19:04.962388Z",
     "iopub.status.busy": "2024-02-04T14:19:04.961579Z",
     "iopub.status.idle": "2024-02-04T14:19:04.968301Z",
     "shell.execute_reply": "2024-02-04T14:19:04.967324Z",
     "shell.execute_reply.started": "2024-02-04T14:19:04.962352Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CE = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def contrastive_loss(v1, v2):\n",
    "    logits = torch.matmul(v1, torch.transpose(v2, 0, 1))\n",
    "    labels = torch.arange(logits.shape[0], device=v1.device)\n",
    "    return CE(logits, labels) + CE(torch.transpose(logits, 0, 1), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T14:19:04.995047Z",
     "iopub.status.busy": "2024-02-04T14:19:04.994290Z",
     "iopub.status.idle": "2024-02-04T14:19:12.883213Z",
     "shell.execute_reply": "2024-02-04T14:19:12.882283Z",
     "shell.execute_reply.started": "2024-02-04T14:19:04.995015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    model_name=model_name,\n",
    "    num_node_features=300,\n",
    "    nout=nout,\n",
    "    nhid=300,\n",
    "    graph_hidden_channels=300,\n",
    ")  # nout = bert model hidden dim\n",
    "\n",
    "# Check if multiple GPUs are available\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     model = nn.DataParallel(model)\n",
    "\n",
    "save_path = \"/kaggle/input/altegrad-weights/best_model_lrap_gcn.pt\"\n",
    "print(\"loading best model...\")\n",
    "model.load_state_dict(torch.load(save_path)[\"model_state_dict\"])\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T14:19:12.885228Z",
     "iopub.status.busy": "2024-02-04T14:19:12.884609Z",
     "iopub.status.idle": "2024-02-04T14:19:12.890372Z",
     "shell.execute_reply": "2024-02-04T14:19:12.889576Z",
     "shell.execute_reply.started": "2024-02-04T14:19:12.885193Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Retrieving model parameters\n",
    "graph_encoder_params = list(model.graph_encoder.parameters())\n",
    "text_encoder_params = list(model.text_encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T14:19:12.891945Z",
     "iopub.status.busy": "2024-02-04T14:19:12.891649Z",
     "iopub.status.idle": "2024-02-04T14:19:12.901188Z",
     "shell.execute_reply": "2024-02-04T14:19:12.900361Z",
     "shell.execute_reply.started": "2024-02-04T14:19:12.891920Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_validation_loss = float(\"inf\")\n",
    "best_lrap_score = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T16:03:04.514088Z",
     "iopub.status.busy": "2024-02-04T16:03:04.513013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "nb_epochs = 10\n",
    "text_lr = 3e-5\n",
    "init_lr = 1e-4\n",
    "print_every = 25\n",
    "\n",
    "\n",
    "# Initialize Accelerator and GradScaler\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "scaler = GradScaler()\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    [{\"params\": graph_encoder_params}, {\"params\": text_encoder_params, \"lr\": text_lr}],\n",
    "    lr=init_lr,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Move model and optimizer to device\n",
    "model, optimizer, train_loader, val_loader, scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_loader, val_loader, scheduler\n",
    ")\n",
    "\n",
    "\n",
    "# Define function for training loop\n",
    "def train(model, optimizer, train_loader, scaler):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch.input_ids.to(accelerator.device)\n",
    "        batch.pop(\"input_ids\")\n",
    "        attention_mask = batch.attention_mask.to(accelerator.device)\n",
    "        batch.pop(\"attention_mask\")\n",
    "        graph_batch = batch.to(accelerator.device)\n",
    "        with autocast():\n",
    "            x_graph, x_text = model(graph_batch, input_ids, attention_mask)\n",
    "            current_loss = contrastive_loss(x_graph, x_text)\n",
    "        scaler.scale(current_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        loss += current_loss.item()\n",
    "        if (batch_idx + 1) % print_every == 0:\n",
    "            print(\n",
    "                \"Iteration: {}, Training loss: {:.4f}\".format(\n",
    "                    batch_idx + 1, loss / print_every\n",
    "                )\n",
    "            )\n",
    "            loss = 0\n",
    "\n",
    "\n",
    "# Define function for validation loop\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        graph_embeddings = []\n",
    "        text_embeddings = []\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch.input_ids.to(accelerator.device)\n",
    "            batch.pop(\"input_ids\")\n",
    "            attention_mask = batch.attention_mask.to(accelerator.device)\n",
    "            batch.pop(\"attention_mask\")\n",
    "            graph_batch = batch.to(accelerator.device)\n",
    "            with autocast():\n",
    "                x_graph, x_text = model(graph_batch, input_ids, attention_mask)\n",
    "                current_loss = contrastive_loss(x_graph, x_text)\n",
    "            val_loss += current_loss.item()\n",
    "\n",
    "            graph_embeddings.extend(x.tolist() for x in x_graph)\n",
    "            text_embeddings.extend(x.tolist() for x in x_text)\n",
    "        #         If you prefer to use the dot product directly with PyTorch\n",
    "        #         text_tensor = torch.tensor(text_embeddings)\n",
    "        #         graph_tensor = torch.tensor(graph_embeddings)\n",
    "        #         similarity = torch.matmul(text_tensor, torch.transpose(graph_tensor, 0, 1))\n",
    "        similarity = cosine_similarity(text_embeddings, graph_embeddings)\n",
    "        gt = np.identity(len(similarity))\n",
    "        lrap_score = label_ranking_average_precision_score(gt, similarity)\n",
    "    return val_loss / len(val_loader), lrap_score\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(nb_epochs):\n",
    "    print(\"-----EPOCH {}-----\".format(epoch + 1))\n",
    "    train(model, optimizer, train_loader, scaler)\n",
    "    val_loss, lrap_score = validate(model, val_loader)\n",
    "    print(\"Validation loss:\", val_loss)\n",
    "    print(\"LRAP: \", lrap_score)\n",
    "    scheduler.step()\n",
    "    if val_loss < best_validation_loss:\n",
    "        best_validation_loss = val_loss\n",
    "        print(\"validation loss improoved saving checkpoint...\")\n",
    "        save_path = os.path.join(\"/kaggle/working/\", \"best_model_loss.pt\")\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"validation_loss\": val_loss,\n",
    "                \"validation_lrap\": lrap_score,\n",
    "            },\n",
    "            save_path,\n",
    "        )\n",
    "        print(\"checkpoint saved to: {}\".format(save_path))\n",
    "    if lrap_score > best_lrap_score:\n",
    "        best_lrap_score = lrap_score\n",
    "        print(\"validation loss improoved saving checkpoint...\")\n",
    "        save_path = os.path.join(\"/kaggle/working/\", \"best_model_lrap.pt\")\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"validation_loss\": val_loss,\n",
    "                \"validation_lrap\": lrap_score,\n",
    "            },\n",
    "            save_path,\n",
    "        )\n",
    "        print(\"checkpoint saved to: {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T15:48:00.228961Z",
     "iopub.status.busy": "2024-02-04T15:48:00.228658Z",
     "iopub.status.idle": "2024-02-04T15:48:00.603914Z",
     "shell.execute_reply": "2024-02-04T15:48:00.602799Z",
     "shell.execute_reply.started": "2024-02-04T15:48:00.228934Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Force to recover GPU memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T15:48:00.605674Z",
     "iopub.status.busy": "2024-02-04T15:48:00.605296Z",
     "iopub.status.idle": "2024-02-04T15:48:51.350505Z",
     "shell.execute_reply": "2024-02-04T15:48:51.349437Z",
     "shell.execute_reply.started": "2024-02-04T15:48:00.605648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# save_path = \"/kaggle/working/best_model_lrap.pt\"\n",
    "# print(\"loading best model...\")\n",
    "# model.load_state_dict(torch.load(save_path)[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "graph_model = model.get_graph_encoder()\n",
    "text_model = model.get_text_encoder()\n",
    "\n",
    "test_cids_dataset = GraphDataset(\n",
    "    root=\"./data/\", gt=gt, split=\"test_cids\", processed=\"/kaggle/working/data/\"\n",
    ")\n",
    "test_text_dataset = TextDataset(file_path=\"./data/test_text.txt\", tokenizer=tokenizer)\n",
    "\n",
    "idx_to_cid = test_cids_dataset.get_idx_to_cid()\n",
    "\n",
    "batch_size_test = 32\n",
    "test_loader = DataLoader(test_cids_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "\n",
    "graph_embeddings = []\n",
    "for batch in test_loader:\n",
    "    for output in graph_model(batch.to(device)):\n",
    "        graph_embeddings.append(output.tolist())\n",
    "\n",
    "test_text_loader = TorchDataLoader(\n",
    "    test_text_dataset, batch_size=batch_size_test, shuffle=False\n",
    ")\n",
    "text_embeddings = []\n",
    "for batch in test_text_loader:\n",
    "    for output in text_model(\n",
    "        batch[\"input_ids\"].to(device), attention_mask=batch[\"attention_mask\"].to(device)\n",
    "    ):\n",
    "        text_embeddings.append(output.tolist())\n",
    "\n",
    "similarity = cosine_similarity(text_embeddings, graph_embeddings)\n",
    "\n",
    "solution = pd.DataFrame(similarity)\n",
    "solution[\"ID\"] = solution.index\n",
    "solution = solution[[\"ID\"] + [col for col in solution.columns if col != \"ID\"]]\n",
    "solution.to_csv(\"/kaggle/working/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7592068,
     "datasetId": 4345940,
     "sourceId": 7499138,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 7550573,
     "datasetId": 4339765,
     "sourceId": 7458531,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 7646782,
     "datasetId": 4396351,
     "sourceId": 7552556,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
